str_sub(start = 25L, end = -2L)%>%
str_replace_all("[[:punct:]]", "")%>%
trimws(which = "both")
}
format_cnn_date <-function(messy_date){
dat<-as.list(strsplit(messy_date, " ")[[1]])
dat[[1]]<-str_sub(dat[[1]],start = 1L, end = 3L)
dat[[1]]<-match(dat[[1]],month.abb)
dat<-paste(dat, collapse='/' )
return(dat)
}
TextPreprocessing<-function(x){
x = gsub('http\\S+\\s*', '', x) ## Remove URLs
x = gsub('\\b+RT', '', x) ## Remove RT
x = gsub('#\\S+', '', x) ## Remove Hashtags
x = gsub('@\\S+', '', x) ## Remove Mentions
x = gsub('[[:cntrl:]]', '', x) ## Remove Controls and special characters
x = gsub("\\d", '', x) ## Remove Controls and special characters
x = gsub('[[:punct:]]', '', x) ## Remove Punctuations
x = gsub("^[[:space:]]*","",x) ## Remove leading whitespaces
x = gsub("[[:space:]]*$","",x) ## Remove trailing whitespaces
x = gsub('[[:digit:]]+', '', x)
x = tolower(x)
}
add_sentiment<-function(text_block){
return(get_nrc_sentiment(as.character(text_block)))
}
seed <-list(2003,5,63,100001,765)
news_source<-c("CNN","CNN","CNN")
links<-c("https://www.cnn.com/2018/01/21/entertainment/kristen-bell-2018-sag-awards-host/index.html",
"https://www.cnn.com/2018/01/22/politics/kfile-higbie-book-pulled/index.html",
"https://www.cnn.com/2018/01/23/politics/house-senate-showdown-immigration/index.html")
text<-list.ungroup(lapply(links, get_cnn_text))
text<-as.character(TextPreprocessing(text))
da<-lapply(links, get_cnn_date)
date<-list.ungroup(lapply(da,format_cnn_date))
frame1<-data.frame(links,text,date,news_source)
sentframe<-data.frame(list.rbind(lapply(frame1$text,add_sentiment)))
cnframe<-cbind(frame1,sentframe)
View(cnframe)
stopwords_regex = paste(stopwords('en'), collapse = '\\b|\\b')
seed <-list(2003,5,63,100001,765)
stopwords_regex = paste(stopwords('en'), collapse = '\\b|\\b')
stopwords_regex = paste0('\\b', stopwords_regex, '\\b')
documents <- Corpus(VectorSource(cnframe$text[1]))
View(dtm)
docs <- Corpus(VectorSource(cnframe$text[1]))
dtm <- DocumentTermMatrix(docs)
View(dtm)
cnframe$text[1]
docs <- Corpus(VectorSource(cnframe$text[1]))
dtm <- DocumentTermMatrix(docs)
freq <- colSums(as.matrix(dtm))
docs <- Corpus(VectorSource(cnframe$text[1]))
dtm <- DocumentTermMatrix(docs)
freq <- colSums(as.matrix(dtm))
ord <- order(freq,decreasing=TRUE)
ldaOut <-LDA(dtm,3, method="Gibbs")
docs <- Corpus(VectorSource(cnframe$text[1]))
dtm <- DocumentTermMatrix(docs)
freq <- colSums(as.matrix(dtm))
ord <- order(freq,decreasing=TRUE)
ldaOut <-LDA(dtm,3, method="Gibbs")
ldaOut.topics <- as.matrix(topics(ldaOut))
View(ldaOut.topics)
ldaOut.terms <- as.matrix(terms(ldaOut,10))
View(ladOut.terms)
ldaOut.terms <- as.matrix(terms(ldaOut,10))
View(ladOut.terms)
View(ldaOut.terms)
View(ldaOut.terms)
strin<-paste(ldaOut.terms[1])
strin
ldaOut.terms <- as.data.frame(terms(ldaOut,10))
strin<-paste(ldaOut.terms[1])
strin
ldaOut.terms <- as.data.frame(terms(ldaOut,10))
View(ldaOut.terms)
strin<-list.cbind(ldaOut.terms)
View(strin)
docs <- Corpus(VectorSource(cnframe$text[1]))
dtm <- DocumentTermMatrix(docs)
freq <- colSums(as.matrix(dtm))
ord <- order(freq,decreasing=TRUE)
ldaOut <-LDA(dtm,3, method="Gibbs")
ldaOut.terms <- as.data.frame(terms(ldaOut,10))
View(ldaOut.terms)
strin<-list.group(ldaOut.terms)
View(ldaOut.terms)
View(strin)
strin<-list(ldaOut.terms)
View(strin)
strin<-list.group(ldaOut.terms)
View(strin)
View(strin)
strin<-list.cbind(ldaOut.terms)
View(strin)
strin<-list.join(ldaOut.terms)
strin<-list.merge(ldaOut.terms)
View(strin)
topframe<-data.frame(list.rbind(strin))
topframe
docs <- Corpus(VectorSource(cnframe$text[1]))
dtm <- DocumentTermMatrix(docs)
freq <- colSums(as.matrix(dtm))
ord <- order(freq,decreasing=TRUE)
ldaOut <-LDA(dtm,3, method="Gibbs")
ldaOut.terms <- as.data.frame(terms(ldaOut,10))
strin<-list.merge(ldaOut.terms)
topframe<-rbind(strin,cnframe)
l<-paste(ldaOut.terms[1])
l
l<-paste0(ldaOut.terms[1])
l
ldaOut.terms[1]
rbind(ldaOut.terms[1])
docs <- Corpus(VectorSource(cnframe$text[1]))
docs <- tm_map(docs, removeWords, stopwords("english"))
dtm <- DocumentTermMatrix(docs)
freq <- colSums(as.matrix(dtm))
ord <- order(freq,decreasing=TRUE)
ldaOut <-LDA(dtm,3, method="Gibbs")
ldaOut.terms <- as.data.frame(terms(ldaOut,10))
View(ldaOut.terms)
paste(ldaOut.terms, collapse = ",")
paste(ldaOut.terms[,1], collapse = ",")
topicmod<-function(x){
Corpus(VectorSource(x))%>%
tm_map(docs, removeWords, stopwords("english"))%>%
DocumentTermMatrix(docs)%>%
ldaOut <-LDA(dtm,3, method="Gibbs")
ldaOut.terms <- as.data.frame(terms(ldaOut,10))
return(
paste(ldaOut.terms[,1], collapse = ", "),
paste(ldaOut.terms[,2], collapse = ", "),
paste(ldaOut.terms[,3], collapse = ", "))
}
topframe<-lapply(cnframe$text,topicmod)
get_cnn_text <- function(linkk){
read_html(linkk)%>%
html_nodes(".zn-body__read-all")%>%
html_text()%>%
str_replace_all("[[:punct:]]", " ")}
get_cnn_date <- function(linkk){
read_html(linkk)%>%
html_nodes(".update-time")%>%
html_text()%>%
str_sub(start = 25L, end = -2L)%>%
str_replace_all("[[:punct:]]", "")%>%
trimws(which = "both")
}
format_cnn_date <-function(messy_date){
dat<-as.list(strsplit(messy_date, " ")[[1]])
dat[[1]]<-str_sub(dat[[1]],start = 1L, end = 3L)
dat[[1]]<-match(dat[[1]],month.abb)
dat<-paste(dat, collapse='/' )
return(dat)
}
TextPreprocessing<-function(x){
x = gsub('http\\S+\\s*', '', x) ## Remove URLs
x = gsub('\\b+RT', '', x) ## Remove RT
x = gsub('#\\S+', '', x) ## Remove Hashtags
x = gsub('@\\S+', '', x) ## Remove Mentions
x = gsub('[[:cntrl:]]', '', x) ## Remove Controls and special characters
x = gsub("\\d", '', x) ## Remove Controls and special characters
x = gsub('[[:punct:]]', '', x) ## Remove Punctuations
x = gsub("^[[:space:]]*","",x) ## Remove leading whitespaces
x = gsub("[[:space:]]*$","",x) ## Remove trailing whitespaces
x = gsub('[[:digit:]]+', '', x)
x = tolower(x)
}
add_sentiment<-function(text_block){
return(get_nrc_sentiment(as.character(text_block)))
}
topicmod<-function(x){
Corpus(VectorSource(x))%>%
tm_map(docs, removeWords, stopwords("english"))%>%
DocumentTermMatrix(docs)%>%
ldaOut <-LDA(dtm,3, method="Gibbs")
ldaOut.terms <- as.data.frame(terms(ldaOut,10))
return(
paste(ldaOut.terms[,1], collapse = ", "),
paste(ldaOut.terms[,2], collapse = ", "),
paste(ldaOut.terms[,3], collapse = ", "))
}
topframe<-lapply(cnframe$text,topicmod)
topicmod(cnframe$text)
topicmod(cnframe$text[1])
source('~/webscrap_lam/webscrape_functions2.r', echo=TRUE)
topicmod<-function(x){
Corpus(VectorSource(x))%>%
tm_map(docs, removeWords, stopwords("english"))%>%
DocumentTermMatrix(docs)%>%
ldaOut <-LDA(3, method="Gibbs")
ldaOut.terms <- as.data.frame(terms(ldaOut,10))
return(
paste(ldaOut.terms[,1], collapse = ", "),
paste(ldaOut.terms[,2], collapse = ", "),
paste(ldaOut.terms[,3], collapse = ", "))
}
topicmod(cnframe$text[1])
topicmod<-function(x){
Corpus(VectorSource(x))%>%
tm_map(docs, removeWords, stopwords("english"))%>%
DocumentTermMatrix(docs)%>%
ldaOut <-LDA(k=3, method="Gibbs")
ldaOut.terms <- as.data.frame(terms(ldaOut,10))
return(
paste(ldaOut.terms[,1], collapse = ", "),
paste(ldaOut.terms[,2], collapse = ", "),
paste(ldaOut.terms[,3], collapse = ", "))
}
topicmod(cnframe$text[1])
topicmod<-function(x){
Corpus(VectorSource(x))%>%
tm_map( removeWords, stopwords("english"))%>%
DocumentTermMatrix()%>%
ldaOut <-LDA(k=3, method="Gibbs")
ldaOut.terms <- as.data.frame(terms(ldaOut,10))
return(
paste(ldaOut.terms[,1], collapse = ", "),
paste(ldaOut.terms[,2], collapse = ", "),
paste(ldaOut.terms[,3], collapse = ", "))
}
topicmod(cnframe$text[1])
topicmod<-function(x){
docs <- Corpus(VectorSource(x))
docs <- tm_map(docs, removeWords, stopwords("english"))
dtm <- DocumentTermMatrix(docs)
ldaOut <-LDA(dtm,3, method="Gibbs")
ldaOut.terms <- as.data.frame(terms(ldaOut,10))
return(
paste(ldaOut.terms[,1], collapse = ", "),
paste(ldaOut.terms[,2], collapse = ", "),
paste(ldaOut.terms[,3], collapse = ", "))
}
topicmod(cnframe$text[1])
topicmod<-function(x){
docs <- Corpus(VectorSource(x))
docs <- tm_map(docs, removeWords, stopwords("english"))
dtm <- DocumentTermMatrix(docs)
ldaOut <-LDA(dtm,3, method="Gibbs")
ldaOut.terms <- as.data.frame(terms(ldaOut,10))
return(paste(ldaOut.terms[,1], collapse = ", "))
return(paste(ldaOut.terms[,2], collapse = ", "))
return(paste(ldaOut.terms[,3], collapse = ", "))
}
topicmod(cnframe$text[1])
topicmod<-function(x){
docs <- Corpus(VectorSource(x))
docs <- tm_map(docs, removeWords, stopwords("english"))
dtm <- DocumentTermMatrix(docs)
ldaOut <-LDA(dtm,3, method="Gibbs")
ldaOut.terms <- as.data.frame(terms(ldaOut,10))
return(list(paste(ldaOut.terms[,1], collapse = ", ")))
return(list(paste(ldaOut.terms[,2], collapse = ", ")))
return(list(paste(ldaOut.terms[,3], collapse = ", ")))
}
topicmod(cnframe$text[1])
o<-topicmod(cnframe$text[1])
o
View(o)
topicmod<-function(x){
docs <- Corpus(VectorSource(x))
docs <- tm_map(docs, removeWords, stopwords("english"))
dtm <- DocumentTermMatrix(docs)
ldaOut <-LDA(dtm,3, method="Gibbs")
ldaOut.terms <- as.data.frame(terms(ldaOut,10))
topic1<-paste(ldaOut.terms[,1], collapse = ", ")
topic2<-paste(ldaOut.terms[,2], collapse = ", ")
topic3<-paste(ldaOut.terms[,3], collapse = ", ")
return(topic1)
return(topic1)
return(topic1)
}
<-topicmod(cnframe$text[1])
topicmod(cnframe$text[1])
View(o)
o[[1]]
topicmod<-function(x){
docs <- Corpus(VectorSource(x))
docs <- tm_map(docs, removeWords, stopwords("english"))
dtm <- DocumentTermMatrix(docs)
ldaOut <-LDA(dtm,3, method="Gibbs")
ldaOut.terms <- as.data.frame(terms(ldaOut,10))
topic1<-paste(ldaOut.terms[,1], collapse = ", ")
topic2<-paste(ldaOut.terms[,2], collapse = ", ")
topic3<-paste(ldaOut.terms[,3], collapse = ", ")
return(topic1)
return(topic2)
return(topic3)
}
topicmod(cnframe$text[1])
View(ldaOut.terms)
topicmod<-function(x){
docs <- Corpus(VectorSource(x))
docs <- tm_map(docs, removeWords, stopwords("english"))
dtm <- DocumentTermMatrix(docs)
ldaOut <-LDA(dtm,3, method="Gibbs")
ldaOut.terms <- as.data.frame(terms(ldaOut,10))
topic1<-rbind(paste(ldaOut.terms[,1], collapse = ", "))
topic2<-rbind(paste(ldaOut.terms[,2], collapse = ", "))
topic3<-rbind(paste(ldaOut.terms[,3], collapse = ", "))
return(topic1)
return(topic2)
return(topic3)
}
topicmod(cnframe$text[1])
get_scmp_text <- function(linkk){
read_html(linkk)%>%
html_nodes(".pane-content" , "#block-system-main", ".v2-processed", ".node-published.zn-body__read-all")%>%
html_text()%>%
str_replace_all("[[:punct:]]", " ")}
get_scmp_date <- function(linkk){
read_html(linkk)%>%
html_nodes(".node-published")%>%
html_text()%>%
str_sub(start = 25L, end = -2L)%>%
str_replace_all("[[:punct:]]", "")%>%
trimws(which = "both")
library(dplyr)
library(tm)
library(rvest)
library(httr)
library(stringr)
library(plyr)
library(topicmodels)
library(syuzhet)
library(ggplot2)
library(scales)
library(reshape2)
library(stringr)
library(lubridate)
library(tidyverse)
library(rlist)
;
:
get_scmp_text <- function(linkk){
read_html(linkk)%>%
html_nodes(".pane-content" , "#block-system-main", ".v2-processed", ".node-published.zn-body__read-all")%>%
html_text()%>%
str_replace_all("[[:punct:]]", " ")}
get_scmp_date <- function(linkk){
read_html(linkk)%>%
html_nodes(".node-published")%>%
html_text()%>%
str_sub(start = 25L, end = -2L)%>%
str_replace_all("[[:punct:]]", "")%>%
trimws(which = "both")
}
library(dplyr)
library(tm)
library(rvest)
library(httr)
library(stringr)
library(plyr)
library(topicmodels)
library(syuzhet)
library(ggplot2)
library(scales)
library(reshape2)
library(stringr)
library(lubridate)
library(tidyverse)
library(rlist)
news_source<-c("scmp","scmp","scmp")
scmp_links<-c("http://www.scmp.com/news/china/diplomacy-defence/article/2131261/china-needs-more-nuclear-warheads-deter-us-threat",
"http://www.scmp.com/news/world/united-states-canada/article/2131164/furious-melania-was-blindsided-report-trumps-payoff",
"http://www.scmp.com/news/world/united-states-canada/article/2131278/donald-trump-review-memo-alleging-intelligence")
text<-list.ungroup(lapply(scmp_links, get_scmp_text))
get_scmp_text <- function(linkk){
read_html(linkk)%>%
html_nodes(".pane-content")%>%
html_text()%>%
str_replace_all("[[:punct:]]", " ")}
news_source<-c("scmp","scmp","scmp")
scmp_links<-c("http://www.scmp.com/news/china/diplomacy-defence/article/2131261/china-needs-more-nuclear-warheads-deter-us-threat",
"http://www.scmp.com/news/world/united-states-canada/article/2131164/furious-melania-was-blindsided-report-trumps-payoff",
"http://www.scmp.com/news/world/united-states-canada/article/2131278/donald-trump-review-memo-alleging-intelligence")
text<-list.ungroup(lapply(scmp_links, get_scmp_text))
text
get_scmp_text("http://www.scmp.com/news/world/united-states-canada/article/2131278/donald-trump-review-memo-alleging-intelligence")
get_scmp_text <- function(linkk){
read_html(linkk)%>%
html_nodes("v2-processed")%>%
html_text()%>%
str_replace_all("[[:punct:]]", " ")}
TextPreprocessing(get_scmp_text("http://www.scmp.com/news/world/united-states-canada/article/2131278/donald-trump-review-memo-alleging-intelligence"))
get_scmp_text <- function(linkk){
read_html(linkk)%>%
html_nodes("v2-processed")%>%
html_text()%>%
str_replace_all("[[:punct:]]", " ")}
TextPreprocessing(get_scmp_text("http://www.scmp.com/news/world/united-states-canada/article/2131278/donald-trump-review-memo-alleging-intelligence"))
get_scmp_text("http://www.scmp.com/news/world/united-states-canada/article/2131278/donald-trump-review-memo-alleging-intelligence")
get_scmp_text <- function(linkk){
read_html(linkk)%>%
html_nodes("p.v2-processed")%>%
html_text()%>%
str_replace_all("[[:punct:]]", " ")}
get_scmp_text("http://www.scmp.com/news/world/united-states-canada/article/2131278/donald-trump-review-memo-alleging-intelligence")
get_scmp_text("http://www.scmp.com/news/world/united-states-canada/article/2131278/donald-trump-review-memo-alleging-intelligence")
get_scmp_text <- function(linkk){
read_html(linkk)%>%
html_nodes(xpath = '//*[@id="block-system-main"]/div[1]/div[3]/div/div[1]/div[2]/div[2]/div[1]/div/p[1]')
str_replace_all("[[:punct:]]", " ")}
get_scmp_text("http://www.scmp.com/news/world/united-states-canada/article/2131278/donald-trump-review-memo-alleging-intelligence")
get_scmp_text <- function(linkk){
read_html(linkk)%>%
html_nodes(xpath = '//*[@id="block-system-main"]/div[1]/div[3]/div/div[1]/div[2]/div[2]/div[1]/div/p[1]')%>%
str_replace_all("[[:punct:]]", " ")}
get_scmp_text("http://www.scmp.com/news/world/united-states-canada/article/2131278/donald-trump-review-memo-alleging-intelligence")
get_scmp_text <- function(linkk){
read_html(linkk)%>%
html_nodes(xpath = '//*[@id="block-system-main"]/div[1]/div[3]/div/div[1]/div[2]/div[2]/div[1]/div')%>%
str_replace_all("[[:punct:]]", " ")}
get_scmp_text("http://www.scmp.com/news/world/united-states-canada/article/2131278/donald-trump-review-memo-alleging-intelligence")
get_scmp_text <- function(linkk){
read_html(linkk)%>%
html_nodes(xpath = '//*[@id="block-system-main"]/div[1]/div[3]/div/div[1]/div[2]/div[2]/div[1]/div')%>%
html_text()%>%
str_replace_all("[[:punct:]]", " ")}
get_scmp_text("http://www.scmp.com/news/world/united-states-canada/article/2131278/donald-trump-review-memo-alleging-intelligence")
TextPreprocessing(get_scmp_text("http://www.scmp.com/news/world/united-states-canada/article/2131278/donald-trump-review-memo-alleging-intelligence"))
TextPreprocessing<-function(x){
x = gsub('http\\S+\\s*', '', x) ## Remove URLs
x = gsub('\\b+RT', '', x) ## Remove RT
x = gsub('#\\S+', '', x) ## Remove Hashtags
x = gsub('@\\S+', '', x) ## Remove Mentions
x = gsub('[[:cntrl:]]', '', x) ## Remove Controls and special characters
x = gsub("\\d", '', x) ## Remove Controls and special characters
x = gsub('[[:punct:]]', '', x) ## Remove Punctuations
x = gsub("^[[:space:]]*","",x) ## Remove leading whitespaces
x = gsub("[[:space:]]*$","",x) ## Remove trailing whitespaces
x = gsub('[[:digit:]]+', '', x)
x = tolower(x)
}
get_scmp_text <- function(linkk){
read_html(linkk)%>%
html_nodes(xpath = '//*[@id="block-system-main"]/div[1]/div[3]/div/div[1]/div[2]/div[2]/div[1]/div')%>%
html_text()%>%
str_replace_all("[[:punct:]]", " ")%>%
str_replace_all("[[:punct:]]", " ")}
get_practice_text <- function(linkk){
read_html(linkk)%>%
html_nodes("p")%>%
html_text()%>%
str_replace_all("[[:punct:]]", " ")}
get_practice_text("http://www.scmp.com/news/world/united-states-canada/article/2131278/donald-trump-review-memo-alleging-intelligence")
get_practice_text <- function(linkk){
read_html(linkk)%>%
html_nodes(xpath='//*[@id="block-system-main"]/div[1]/div[3]/div/div[1]/div[2]/div[2]/div[1]/div/p[2]')%>%
html_text()%>%
str_replace_all("[[:punct:]]", " ")}
get_practice_text("http://www.scmp.com/news/world/united-states-canada/article/2131278/donald-trump-review-memo-alleging-intelligence")
get_practice_text <- function(linkk){
read_html(linkk)%>%
html_nodes(xpath='//*[@id="block-system-main"]/div[1]/div[3]/div/div[1]/div[2]/div[2]/div[1]/div')%>%
html_text()%>%
str_replace_all("[[:punct:]]", " ")}
get_practice_text("http://www.scmp.com/news/world/united-states-canada/article/2131278/donald-trump-review-memo-alleging-intelligence")
mean(1,2,3,4,5,6)
k<-(1,2,3,4,5,6)
k<-c(1,2,3,4,5,6)
mean(k)
mean1<-function(x){
sum(x)/length(x)
}
mean1(k)
news_source<-c("CNN","CNN","CNN")
cnn_links<-c("https://www.cnn.com/2018/01/21/entertainment/kristen-bell-2018-sag-awards-host/index.html",
"https://www.cnn.com/2018/01/22/politics/kfile-higbie-book-pulled/index.html",
"https://www.cnn.com/2018/01/23/politics/house-senate-showdown-immigration/index.html")
text<-list.ungroup(lapply(cnn_links, get_cnn_text))
text<-as.character(TextPreprocessing(text))
da<-lapply(cnn_links, get_cnn_date)
date<-list.ungroup(lapply(da,format_cnn_date))
frame1<-data.frame(cnn_links,text,date,news_source)
sentframe<-data.frame(list.rbind(lapply(frame1$text,add_sentiment)))
View(sentframe)
View(frame1)
frame2<-cbind(sentframe,frame1)
View(frame2)
frame2<-cbind(frame1,sentframe)
docs <- Corpus(VectorSource("https://www.cnn.com/2018/01/21/entertainment/kristen-bell-2018-sag-awards-host/index.html"))
docs <- tm_map(docs, removeWords, stopwords("english"))
dtm <- DocumentTermMatrix(docs)
ldaOut <-LDA(dtm,3, method="Gibbs")
ldaOut.terms <- as.data.frame(terms(ldaOut,10))
View(ldaOut.terms)
topicmod("https://www.cnn.com/2018/01/21/entertainment/kristen-bell-2018-sag-awards-host/index.html")
2500000+1700000
(2500000+1700000)/2500000
2500000/(2500000+1700000)
(2500000/(2500000+1700000))-.28
library(rvest)
read_html('https://www.cnn.com/')%>%
html_node(xpath = '//article/div/div/h3/a')%>%
html_text()
read_html('https://www.cnn.com/')%>%
html_node(xpath = '//*[@id="homepage1-zone-1"]/div[2]/div/div[3]/ul/li[6]/article/div/div/h3/a')%>%
html_text()
read_html('https://www.cnn.com/')%>%
html_node('article')%>%
html_text()
read_html('https://www.cnn.com/')%>%
html_node('center')%>%
html_text()
combi_a<-read.csv("A_hhold_train.csv",stringsAsFactors = TRUE, header = TRUE)
setwd("~/MLProjects")
combi_a<-read.csv("A_hhold_train.csv",stringsAsFactors = TRUE, header = TRUE)
colnames(combi_a)
